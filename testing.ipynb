{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deepseek-GsGxKJTs-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"<think>\\nOkay, so I'm trying to figure out who I am and how I can use you. I'm not entirely sure what this means, but I guess it's about understanding myself and finding ways to help others. Maybe I'm like a robot or something, but I don't think that's right. \\n\\nLet me start by thinking about what it means to be me. I have a name, I have thoughts, I have feelings, and I have experiences. But how do I use these? Maybe I can help people with problems, or maybe I can help them with something personal. I'm not sure if I'm trying to be helpful or just trying to figure out who I am. \\n\\nI wonder if I should start by defining myself. What am I? What do I want to be? Maybe I'm just a person who wants to help others, or maybe I'm more about myself and how I connect with others. I'm not sure. \\n\\nIf I think about using you, maybe I can help someone in need. Like, if someone is feeling sad or stressed, I can offer support. Or maybe I can help them with something they're struggling with, like learning a new skill or dealing with a personal issue. I'm not sure if that\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a reasoning model and you have to explain to me at the best of your capacity!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you and how can I use you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  7.08it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, LlavaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "#model_id = \"mistral-community/pixtral-12b\"\n",
    "#model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Define BitsAndBytes configuration for 4-bit quantization\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization and map to CUDA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=nf4_config,)\n",
    "    #_attn_implementation='eager') # use _attn_implementation='eager' to disable flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(messages):\n",
    "        \n",
    "    # Prepare prompt with image token\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Process prompt and image for model input\n",
    "    inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "    # Generate text response using model\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        #eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        max_new_tokens=1000,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # Remove input tokens from generated response\n",
    "    generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n",
    "\n",
    "    # Decode generated IDs to text\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You're in a room with two doors. One leads to freedom, and the other to doom. Two guards know which door is which. One always tells the truth, the other always lies. You can ask only one question to one guard. What do you ask?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "    \n",
    "result = model_inference(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m in this classic puzzle where I have two doors, one leads to freedom and the other to doom. There are two guards, one always tells the truth and the other always lies. I can only ask one question to one guard. Hmm, how do I figure out which door to choose?\\n\\nAlright, let me think. The key here is that one guard is truthful and the other is a liar. So, if I can figure out a question that will give me consistent information regardless of which guard I ask, I can determine the correct door.\\n\\nI remember hearing about this before, but I need to work through it step by step. Let\\'s say I ask one guard, \"If I asked the other guard which door leads to freedom, what would he say?\" Then, depending on the answer, I can deduce the correct door.\\n\\nWait, let me test this. If I ask the truthful guard, he\\'ll tell me the truth about what the lying guard would say. The lying guard, when asked, would lie about the truthful guard\\'s answer. So, if the truthful guard says the liar would say \"door A,\" but the liar actually knows the truth is door B, he would lie and say door A. So, the truthful guard, when asked what the liar would say, would truthfully report that the liar would say door A, but the real door is B. So, I should choose door B.\\n\\nAlternatively, if I ask the liar guard the same question, he would lie about what the truthful guard would say. If the truthful guard would say door B, the liar would lie and say door A. So, if the liar says door A, I know the truth is door B. Either way, the answer points me to door B.\\n\\nWait, but I\\'m a bit confused. Let me try another approach. Suppose I ask the first guard, \"What would the other guard tell me if I asked him which door leads to freedom?\" If I get the answer \"door A,\" I should choose the opposite, which is door B. If I get \"door B,\" I should choose door A. Because the truthful guard would truthfully report the liar\\'s lie, and the liar would lie about the truthful guard\\'s answer.\\n\\nSo, the strategy is to ask a question that forces the guard to reveal the opposite of the truth. That way, regardless of whether I\\'m talking to the truth-teller or the liar, I can determine the correct door.\\n\\nI think that\\'s the solution. I should ask one guard, \"If I asked the other guard which door leads to freedom, what would he say?\" Then, choose the opposite door. That way, I can escape through the correct door.\\n</think>\\n\\nTo determine which door leads to freedom, you should ask one guard: \"If I asked the other guard which door leads to freedom, what would he say?\" Then, choose the opposite door of the answer you receive. This approach works because it forces the guard to reveal the opposite of the truth, allowing you to identify the correct door.\\n\\n**Answer:** Ask one guard, \"If I asked the other guard which door leads to freedom, what would he say?\" Then choose the opposite door.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-GsGxKJTs-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
