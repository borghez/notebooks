{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrea.Bagante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import lxml.html\n",
    "import nltk\n",
    "import shutil\n",
    "import orjsonl\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from lxml_html_clean import Cleaner\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = orjsonl.stream('data/open-australian-legal-corpus/corpus.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202260it [00:27, 7404.82it/s] \n"
     ]
    }
   ],
   "source": [
    "#500: 0.27 #10000: \n",
    "DOCUMENT_COUNT = 10000  #5000\n",
    "\n",
    "text = []\n",
    "for index, document in tqdm(enumerate(corpus)):\n",
    "    if index < DOCUMENT_COUNT:\n",
    "        text.append(document['text'])\n",
    "text_df = pd.DataFrame(data={'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Residential Tenancy (Smoke Alarms) Regulations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explosives Act 2012\\n\\nAn Act to provide for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Radiation Protection Act 2005\\n\\nAn Act for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FEDERAL COURT OF AUSTRALIA\\n\\nPEP Community Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DECISION NO: 67/96\\nINDUSTRIAL LAW - INDEPENDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Residential Tenancy (Smoke Alarms) Regulations...\n",
       "1  Explosives Act 2012\\n\\nAn Act to provide for t...\n",
       "2  Radiation Protection Act 2005\\n\\nAn Act for th...\n",
       "3  FEDERAL COURT OF AUSTRALIA\\n\\nPEP Community Se...\n",
       "4  DECISION NO: 67/96\\nINDUSTRIAL LAW - INDEPENDE..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def combine_json(path):\n",
    "#    \n",
    "#    folder_path=path\n",
    "#\n",
    "#    dataframes = []\n",
    "#    lungz = []\n",
    "#\n",
    "#    for filename in os.listdir(folder_path):\n",
    "#        if filename.endswith('.json'):\n",
    "#            file_path = os.path.join(folder_path, filename)\n",
    "#            with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "#                data = json.load(file)\n",
    "#                if \"params\" in data:\n",
    "#                    df = pd.DataFrame(data[\"params\"][\"documents\"]).apply(pd.Series)\n",
    "#                else: \n",
    "#                    df = pd.DataFrame(data)\n",
    "#\n",
    "#                lungh = df.shape[0]\n",
    "#                lungz.append(lungh)\n",
    "#                dataframes.append(df)\n",
    "#\n",
    "#    if dataframes:\n",
    "#        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "#    else:\n",
    "#        print('Attenzione, nessun dataframe creato')\n",
    "#\n",
    "#    if 'super_category_id' in combined_df.columns:\n",
    "#        combined_df['super_category_id'] = combined_df['super_category_id'].astype(int)\n",
    "#\n",
    "#    return combined_df\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = '.\\data'\n",
    "#combined_df = combine_json(folder_path)\n",
    "#combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = combined_df[\"paragraph_body\"]\n",
    "#texts = texts.to_list()\n",
    "#texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_df['text'].to_list()\n",
    "#texts = [text.lower() for text in texts]\n",
    "#texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to train our own tokenizer and build a vocabulary for our corpus. We will be choosing BertWordPieceTokenizer from tokenizers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    " \n",
    "bwpt = tokenizers.BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        strip_accents=True,\n",
    "        lowercase=True,\n",
    ")\n",
    " \n",
    "#filepath = \"input file directory\"\n",
    "\n",
    "bwpt.train_from_iterator(\n",
    "    texts,\n",
    "    vocab_size=10000, #10000\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000 #1000\n",
    ")\n",
    "\n",
    "#doc=5000, vocab=1000: 7.7\n",
    "#doc=10000, vocab=1000: 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scratch/OmniaBERT-vocab.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwpt.save('scratch/OmniaBERT')\n",
    "bwpt.save_model('scratch/', 'OmniaBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import BertTokenizer, LineByLineTextDataset\n",
    "\n",
    "tokenizer_path = 'scratch/OmniaBERT-vocab.txt' \n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['federal', '[MASK]', '(', 'court', ')', 'of', 'australia']\n",
      "Token IDs: [2, 1591, 4, 12, 495, 13, 278, 891, 3]\n",
      "Tokens: [CLS] federal [MASK] ( court ) of australia [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"federal [MASK] (court) of australia\"\n",
    "\n",
    "encoding = tokenizer.encode(sentence)\n",
    "\n",
    "print(tokenizer.tokenize(sentence))\n",
    "print(\"Token IDs:\", encoding)\n",
    "print(\"Tokens:\", tokenizer.decode(encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#'''\n",
    "#transformers has a predefined class LineByLineTextDataset()\n",
    "#which reads your text line by line and converts them to tokens\n",
    "#'''\n",
    "#\n",
    "##dataset= LineByLineTextDataset(\n",
    "##    tokenizer = tokenizer,\n",
    "##    file_path = '/kaggle/input/bert-bangla/raw_bangla_for_BERT.txt',\n",
    "##    block_size = 128  # maximum sequence length\n",
    "##)\n",
    "#\n",
    "#dataset = Dataset.from_list([{\"text\": sentence} for sentence in texts])\n",
    "#\n",
    "#print('No. of lines: ', len(dataset)) # No of lines in your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 10000/10000 [07:35<00:00, 21.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "full_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every text...\n",
    "#for text in tqdm(texts):\n",
    "#    curr_len = len(text)\n",
    "#    if curr_len > max_len:\n",
    "#        max_len = curr_len\n",
    "#print('ok')\n",
    "for text in tqdm(texts):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 512,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  \n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "\n",
    "                   )\n",
    "    full_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "#doc=5000, vocab=1000, max_len=256: 4m\n",
    "#doc=10000, vocab=1000, max_len=512: 8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrea.Bagante\\AppData\\Local\\Temp\\ipykernel_2064\\1921582187.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  full_ids = torch.tensor(full_ids, dtype=torch.long)\n",
      "C:\\Users\\Andrea.Bagante\\AppData\\Local\\Temp\\ipykernel_2064\\1921582187.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(attention_masks, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "full_ids = torch.cat(full_ids, dim=0)\n",
    "full_ids = torch.tensor(full_ids, dtype=torch.long)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo le maschere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = full_ids.detach().clone()\n",
    "rand = torch.rand(input_ids.shape)\n",
    "\n",
    "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "mask_arr = (rand < .15) * (input_ids > 2) \n",
    "for i in range(input_ids.shape[0]):\n",
    "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    input_ids[i, selection] = 4  #[MASK] token == 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.cat(input_ids, dim=0)\n",
    "#input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "#input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)\n",
    "#attention_masks = torch.tensor(attention_masks, dtype=torch.float64)\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, full_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(dataset, 'scratch/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = torch.load('scratch/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the configuration of the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  24361972\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=10000, #10000\n",
    "    hidden_size=420, #420\n",
    "    num_hidden_layers=6, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512 #256\n",
    ")\n",
    "#config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    " \n",
    "model = BertForMaskedLM(config)\n",
    "#model = BertForMaskedLM.from_pretained('bert-base-cased')\n",
    "\n",
    "#config = BertConfig(\n",
    "#vocab_size=tokenizer.vocab_size,\n",
    "#hidden_size=512,\n",
    "#num_hidden_layers=6,\n",
    "#num_attention_heads=8,\n",
    "#intermediate_size=2048,\n",
    "#max_position_embeddings=512,\n",
    "#num_labels=len(unique_labels)\n",
    "#)\n",
    "\n",
    "#model = BertForSequenceClassification(config)\n",
    "\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "\n",
    "#data_collator = DataCollatorForLanguageModeling(\n",
    "#    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primo metodo da testare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Valuto se la GPU è disponibile \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[:][0].to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oeh = next(iter(dataloader))\n",
    "#oeh[0]\n",
    "#model.to(device)(oeh[0].to(device), attention_mask=oeh[1].to(device), labels=oeh[2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 4.39\n",
      "  Training epoch took: 77.33177328109741\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.76\n",
      "  Training epoch took: 77.06180214881897\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.22\n",
      "  Training epoch took: 77.80293679237366\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.11\n",
      "  Training epoch took: 78.26556134223938\n",
      "\n",
      "Training complete!\n",
      "Total training took 310.67945981025696 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loops\n",
    "\n",
    "epochs = 4  #4\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "\n",
    "    # Training\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Unpack del batch di training dal dataloader, i tensori vengono copiati sulla GPU \n",
    "        # il batch contiene due tensori':\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)   \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clippa la norma del gradiente a 1.0. Serve per prevenire il problema dell'\"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #da capire questo\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calcolo della loss media attraverso tutti i batches\n",
    "    avg_train_loss = total_train_loss / len(dataloader)            \n",
    "    \n",
    "    # Controllo quanto tempo ha impiegato questa epoca di allenamento.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'scratch/model_OmniaBERT'\n",
    "\n",
    "if os.path.exists(save_directory):\n",
    "    shutil.rmtree(save_directory)\n",
    " \n",
    "\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(save_directory)\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10326690971851349,\n",
       "  'token': 278,\n",
       "  'token_str': 'of',\n",
       "  'sequence': 'federal court of australia'},\n",
       " {'score': 0.046051524579524994,\n",
       "  'token': 0,\n",
       "  'token_str': '[PAD]',\n",
       "  'sequence': 'federal court australia'},\n",
       " {'score': 0.025952322408556938,\n",
       "  'token': 273,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'federal court the australia'},\n",
       " {'score': 0.02569030225276947,\n",
       "  'token': 300,\n",
       "  'token_str': 'and',\n",
       "  'sequence': 'federal court and australia'},\n",
       " {'score': 0.02100345864892006,\n",
       "  'token': 12,\n",
       "  'token_str': '(',\n",
       "  'sequence': 'federal court ( australia'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"federal court [MASK] australia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondo metodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='scratch/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset # type: ignore\n",
    "    )\n",
    "\n",
    "\n",
    "#training_args = TrainingArguments(\n",
    "#  output_dir='./scratch/',\n",
    "#  evaluation_strategy=\"epoch\",\n",
    "#  learning_rate=2e-5,\n",
    "#  per_device_train_batch_size=16,\n",
    "#  #per_device_eval_batch_size=16,\n",
    "#  num_train_epochs=3,\n",
    "#  weight_decay=0.01,\n",
    "#)\n",
    "#\n",
    "#trainer = Trainer(\n",
    "#  model=model,\n",
    "#  args=training_args,\n",
    "#  train_dataset=final_df[\"paragraph_text\"],\n",
    "#  #eval_dataset=tokenized_datasets[\"test\"],\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model('scratch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('scratch/')\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained('/scratch/')\n",
    "\n",
    "#classificazione = pipeline(\n",
    "#    \"text-classification\",\n",
    "#    model=model,\n",
    "#    tokenizer=tokenizer\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"Federal [MASK] of Australia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model-omnia-QHASXsB0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
