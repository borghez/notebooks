{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.agents import HfApiEngine\n",
    "\n",
    "llm_model = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "llm_engine = HfApiEngine(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "web search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import ReactCodeAgent, ReactJsonAgent, ManagedAgent\n",
    "from transformers.agents.search import DuckDuckGoSearchTool, VisitWebpageTool\n",
    "\n",
    "web_agent = ReactJsonAgent(tools=[DuckDuckGoSearchTool(), VisitWebpageTool()], llm_engine=llm_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_web_agent = ManagedAgent(\n",
    "    agent=web_agent,\n",
    "    name=\"search\",\n",
    "    description=\"Runs web searches for you. Give it your query as an argument.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knowledge base retriever agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2647/2647 [00:30<00:00, 86.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]}) for doc in knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split docs and keep only unique ones\n",
    "print(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_docs):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[new_doc.page_content] = True\n",
    "            docs_processed.append(new_doc)\n",
    "\n",
    "print(\"Embedding documents...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "huggingface_doc_vector_db = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import Tool\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, vectordb: VectorStore, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.vectordb.similarity_search(\n",
    "            query,\n",
    "            k=7,\n",
    "        )\n",
    "\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_doc_retriever_tool = RetrieverTool(huggingface_doc_vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT issues retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "GITHUB_ACCESS_TOKEN = getpass(\"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\") #30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(repo=\"huggingface/peft\", access_token=\"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\", include_prs=False, state=\"all\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_issues_vector_db = FAISS.from_documents(chunked_docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_issues_retriever_tool = RetrieverTool(peft_issues_vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Retriever agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_agent = ReactJsonAgent(\n",
    "    tools=[huggingface_doc_retriever_tool, peft_issues_retriever_tool],\n",
    "    llm_engine=llm_engine,\n",
    "    max_iterations=4,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_retriever_agent = ManagedAgent(\n",
    "    agent=retriever_agent,\n",
    "    name=\"retriever\",\n",
    "    description=\"Retrieves documents from the knowledge base for you that are close to the input query. Give it your query as an argument. The knowledge base includes Hugging Face documentation and PEFT issues.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://sergiopaniego-promptist.hf.space ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since `api_name` was not defined, it was automatically set to the first avilable API: `/predict`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import load_tool, CodeAgent\n",
    "\n",
    "prompt_generator_tool = Tool.from_space(\n",
    "    \"sergiopaniego/Promptist\", name=\"generator_tool\", description=\"Optimizes user input into model-preferred prompts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're loading a tool from the Hub from None. Please make sure this is a source that you trust as the code within that tool will be executed on your machine. Always verify the code of the tools that you load. We recommend specifying a `revision` to ensure you're loading the code that you have checked.\n",
      "TextToImageTool implements a different description in its configuration and class. Using the tool configuration description.\n"
     ]
    }
   ],
   "source": [
    "image_generation_tool = load_tool(\"m-ric/text-to-image\", cache=False)\n",
    "image_generation_agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_image_generation_agent = ManagedAgent(\n",
    "    agent=image_generation_agent,\n",
    "    name=\"image_generation\",\n",
    "    description=\"Generates images from text prompts. Give it your prompt as an argument.\",\n",
    "    additional_prompting=\"\\n\\nYour final answer MUST BE only the generated image location.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the general agent manager to orchestrate the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_agent = ReactCodeAgent(\n",
    "    tools=[],\n",
    "    llm_engine=llm_engine,\n",
    "    managed_agents=[managed_web_agent, managed_retriever_agent, managed_image_generation_agent],\n",
    "    additional_authorized_imports=[\"time\", \"datetime\", \"PIL\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mHow many years ago was Stripe founded?\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I will use the `search` tool to find the founding year of Stripe and then calculate the number of years since it was founded.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mfounding_year_result\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7msearch\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mWhen was Stripe founded\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mFounding year result:\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mfounding_year_result\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'search'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "When was Stripe founded\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I need to find out when Stripe was founded. I will start by performing a web search.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'web_search' with arguments: {'query': 'When was Stripe founded'}\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I have found relevant information on the founding date of Stripe. Now I will visit the first link to get more detailed information.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'visit_webpage' with arguments: {'url': 'https://en.wikipedia.org/wiki/Stripe,_Inc.'}\u001b[0m\n",
      "\u001b[31;20mError in tool call execution: You must install package `markdownify` to run this tool: for instance run `pip install markdownify`.\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- visit_webpage: Visits a webpage at the given url and returns its content as a markdown string.\n",
      "    Takes inputs: {'url': {'type': 'string', 'description': 'The url of the webpage to visit.'}}\n",
      "    Returns an output of type: string\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\search.py\", line 56, in forward\n",
      "    from markdownify import markdownify\n",
      "ModuleNotFoundError: No module named 'markdownify'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 555, in execute_tool_call\n",
      "    observation = available_tools[tool_name](**arguments)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\tools.py\", line 183, in __call__\n",
      "    outputs = self.forward(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\search.py\", line 58, in forward\n",
      "    raise ImportError(\n",
      "ImportError: You must install package `markdownify` to run this tool: for instance run `pip install markdownify`.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1077, in step\n",
      "    observation = self.execute_tool_call(tool_name, arguments)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 563, in execute_tool_call\n",
      "    raise AgentExecutionError(\n",
      "transformers.agents.agents.AgentExecutionError: Error in tool call execution: You must install package `markdownify` to run this tool: for instance run `pip install markdownify`.\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- visit_webpage: Visits a webpage at the given url and returns its content as a markdown string.\n",
      "    Takes inputs: {'url': {'type': 'string', 'description': 'The url of the webpage to visit.'}}\n",
      "    Returns an output of type: string\n",
      "\u001b[31;20mError in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: wKfL4S-Uaz0PIGkqSpHuJ)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1035, in step\n",
      "    llm_output = self.llm_engine(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 134, in __call__\n",
      "    response = self.generate(messages, stop_sequences, grammar)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 190, in generate\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 892, in chat_completion\n",
      "    data = self.post(model=model_url, json=payload, stream=stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 306, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 477, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: wKfL4S-Uaz0PIGkqSpHuJ)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1039, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: wKfL4S-Uaz0PIGkqSpHuJ)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\n",
      "\u001b[31;20mError in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: cXrAphiztSl4hTvY6xGSg)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1035, in step\n",
      "    llm_output = self.llm_engine(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 134, in __call__\n",
      "    response = self.generate(messages, stop_sequences, grammar)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 190, in generate\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 892, in chat_completion\n",
      "    data = self.post(model=model_url, json=payload, stream=stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 306, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 477, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: cXrAphiztSl4hTvY6xGSg)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1039, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: cXrAphiztSl4hTvY6xGSg)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\n",
      "\u001b[31;20mError in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: SWxTeLV2wyjinreApiBO2)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1035, in step\n",
      "    llm_output = self.llm_engine(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 134, in __call__\n",
      "    response = self.generate(messages, stop_sequences, grammar)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\llm_engine.py\", line 190, in generate\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 892, in chat_completion\n",
      "    data = self.post(model=model_url, json=payload, stream=stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 306, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 477, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: SWxTeLV2wyjinreApiBO2)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "  File \"c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\transformers\\agents\\agents.py\", line 1039, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: SWxTeLV2wyjinreApiBO2)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s).\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: Given the repeated errors with the `visit_webpage` tool, I will not try to use it again. Instead, I will rely on the search results that I obtained earlier to provide as much information as possible about when Stripe was founded.\u001b[0m\n",
      "\u001b[33;1m>>> Calling tool: 'final_answer' with arguments: {'answer': \"### 1. Task outcome (short version):\\nStripe was founded in 2010 by Irish brothers John and Patrick Collison.\\n\\n### 2. Task outcome (extremely detailed version):\\nStripe, Inc. is an Irish-American multinational financial services and software as a service (SaaS) company. It was founded in 2010 by Irish entrepreneurs John and Patrick Collison. The company began as a project while they were still in high school andcollege. They launched publicly in September 2011after a lengthyprivate beta period.In 2011, Stripealreadyacquireda$2 millioninvestment fromavarietyofventurecapitalists,includingPeterThiel,ElonMusk,SequoiaCapital,SVCapital,andAndreesSenhorowitz.\\n\\n### 3. Additional context (if relevant):\\n- Stripe's primary mission is to provide tools for developers to create secure and innovative buying experiences.\\n- The companyhasitsheadquartersinSouthSanFrancisco,CaliforniainAmerica,andDublin,Ireland.\\n- BesidesfoundingStripe,Patrick serves as the CEO and John serves as thePresident.\"}\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mFounding year result: ### 1. Task outcome (short version):\n",
      "Stripe was founded in 2010 by Irish brothers John and Patrick Collison.\n",
      "\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "Stripe, Inc. is an Irish-American multinational financial services and software as a service (SaaS) company. It was founded in 2010 by Irish entrepreneurs John and Patrick Collison. The company began as a project while they were still in high school andcollege. They launched publicly in September 2011after a lengthyprivate beta period.In 2011, Stripealreadyacquireda$2 millioninvestment fromavarietyofventurecapitalists,includingPeterThiel,ElonMusk,SequoiaCapital,SVCapital,andAndreesSenhorowitz.\n",
      "\n",
      "### 3. Additional context (if relevant):\n",
      "- Stripe's primary mission is to provide tools for developers to create secure and innovative buying experiences.\n",
      "- The companyhasitsheadquartersinSouthSanFrancisco,CaliforniainAmerica,andDublin,Ireland.\n",
      "- BesidesfoundingStripe,Patrick serves as the CEO and John serves as thePresident.\n",
      "\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I will extract the founding year from the result and use it to determine the number of years since Stripe was founded. Then I will return the final answer using the `final_answer` tool.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;109;01mimport\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;109mdatetime\u001b[39m\n",
      "\n",
      "\u001b[38;5;7mfounding_year\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2010\u001b[39m\n",
      "\n",
      "\u001b[38;5;7mcurrent_year\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mdatetime\u001b[39m\u001b[38;5;109;01m.\u001b[39;00m\u001b[38;5;7mdatetime\u001b[39m\u001b[38;5;109;01m.\u001b[39;00m\u001b[38;5;7mnow\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[38;5;109;01m.\u001b[39;00m\u001b[38;5;7myear\u001b[39m\n",
      "\u001b[38;5;7myears_since_founding\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcurrent_year\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m-\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mfounding_year\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7myears_since_founding\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20m14\u001b[0m\n",
      "\u001b[32;20;1mFinal answer:\u001b[0m\n",
      "\u001b[32;20m14\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager_agent.run(\"How many years ago was Stripe founded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = manager_agent.run(\n",
    "    \"Improve this prompt, then generate an image of it.\", prompt=\"a black cocker running on a beach at sunset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "result=\"C:\\\\Users\\\\ANDREA~1.BAG\\\\AppData\\\\Local\\\\Temp\\\\tmpvnjruipq\\\\42efda3a-900c-4e8a-b176-b1802c140429.png\"\n",
    "display(Image(filename=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_agent.run(\"How can I push a model to the Hub?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_agent.run(\"How do you combine multiple adapters in peft?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-augmented-sjGhpBXj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
