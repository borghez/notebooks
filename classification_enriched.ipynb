{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrea.Bagante\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import lxml.html\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from lxml_html_clean import Cleaner\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json(path):\n",
    "    \n",
    "    folder_path=path\n",
    "\n",
    "    dataframes = []\n",
    "    lungz = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "                data = json.load(file)\n",
    "                if \"params\" in data:\n",
    "                    df = pd.DataFrame(data[\"params\"][\"documents\"]).apply(pd.Series)\n",
    "                else: \n",
    "                    df = pd.DataFrame(data)\n",
    "\n",
    "                lungh = df.shape[0]\n",
    "                lungz.append(lungh)\n",
    "                dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        print('Attenzione, nessun dataframe creato')\n",
    "\n",
    "    if 'super_category_id' in combined_df.columns:\n",
    "        combined_df['super_category_id'] = combined_df['super_category_id'].astype(int)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def clean_text(text, language: str):\n",
    "\n",
    "    assert isinstance(language, str), 'Strings only!'\n",
    "\n",
    "    sw = stopwords.words(language)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    html=re.compile(r'<.*?>') \n",
    "    #text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    #punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    #for p in punctuations:\n",
    "    #    text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "\n",
    "    regex_compiled = [\n",
    "    r\"(c\\.p\\.?c\\.?)\", ## cpc, c.p.c, etcc (codice procedura civile)\n",
    "    r\"(c\\.c\\.?)\", ## codice civile\n",
    "    r\"([\\W+])(art\\.?)([\\s+|\\d+])\", ## articolo\n",
    "    r\"([\\W+])(artt\\.?)([\\s+|\\d+])\", ## articoli\n",
    "    r\"([a-zA-Z])()(\\d)\", ## stacca lettera da numero\n",
    "    r\"(\\d)()([a-zA-Z])\", ## stacca numero da lettera\n",
    "    r\"(-+)\",\n",
    "    r\"(\\u00A0{2,})\",\n",
    "    r\"(<ul\\.*[^>]*>)(.*?)(</ul>)\",\n",
    "    r\"(<li\\.*[^>]*>)(.*?)(</li>)\",\n",
    "    r\"(<ol\\.*[^>]*>)(.*?)(</ol>)\",\n",
    "    r\"(_______________________________________________________________________________________________________)\",\n",
    "    r\"(_{2,})\",\n",
    "    r\"([a-z]{1})()([A-Z]{1})\",\n",
    "    r\"(<em\\.*[^>]*>)(.*?)(</em>)\"\n",
    "    ]\n",
    "    regex_compiled_subs = [\n",
    "    r\"codice procedura civile\",\n",
    "    r\"codice civile\",\n",
    "    r\"\\1articolo\\3\",\n",
    "    r\"\\1articoli\\3\",\n",
    "    r\"\\1 \\3\",\n",
    "    r\"\\1 \\3\",\n",
    "    r\"\",\n",
    "    r\" [campo] \",\n",
    "    r\"\\n\\2\\n\",\n",
    "    r\"\\n\\2\\n\",\n",
    "    r\"\\n\\2\\n\",\n",
    "    r\" \",\n",
    "    r\" \",\n",
    "    r\"\\1 \\3\",\n",
    "    r\"\\2\"\n",
    "    ]\n",
    "    regex_compiled_spaces = [\n",
    "        r\"(\\s{3,})\", ##spazi in eccesso\n",
    "    r\"(\\n{3,})\" ## almeno un 'a capo'\n",
    "    ]\n",
    "    regex_compiled_spaces_subs = [\n",
    "    r\" \",\n",
    "    r\" \"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for c, s in zip(regex_compiled, regex_compiled_subs):\n",
    "        text = re.compile(c).sub(s, text)\n",
    "    for c, s in zip(regex_compiled_spaces, regex_compiled_spaces_subs):\n",
    "        text = re.compile(c).sub(s, text)\n",
    "\n",
    "    \"\"\"metodo per rimuovere eventualmente dell'html dal testo\"\"\"\n",
    "    cleaner = Cleaner(page_structure=False, links=False)\n",
    "    doc_from_text = lxml.html.fromstring(text)\n",
    "    doc_from_text = cleaner.clean_html(html=doc_from_text)\n",
    "    text = doc_from_text.text_content()\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "#range(0, len(text), max_length) genera una sequenza di numeri che parte da 0 e arriva fino alla lunghezza del testo (len(text)), con incrementi di max_length\n",
    "#Per ogni valore di i generato dal range, viene preso un “chunk” del testo che va dall’indice i all’indice i + max_length.\n",
    "def split_and_chunk_dataframe(df, text_column, id_column, name_column, max_length):\n",
    "    chunks = []\n",
    "    classes = []\n",
    "    names = []\n",
    "\n",
    "    def split_text_into_chunks(text, max_length):\n",
    "        return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text_chunks = split_text_into_chunks(row[text_column], max_length)\n",
    "        chunks.extend(text_chunks)\n",
    "        classes.extend([row[id_column]] * len(text_chunks))\n",
    "        names.extend([row[name_column]] * len(text_chunks))\n",
    "\n",
    "    chunked_df = pd.DataFrame({\n",
    "        text_column: chunks,\n",
    "        id_column: classes,\n",
    "        name_column: names\n",
    "    })\n",
    "    \n",
    "    return chunked_df\n",
    "\n",
    "\n",
    "def balancing_df(df, id_column: str, perce:int):\n",
    "\n",
    "    assert isinstance(id_column, str), 'Strings only!'\n",
    "    assert isinstance(perce, (int, float)), 'It must be a number!'\n",
    "\n",
    "    if isinstance(perce, int):\n",
    "        perce = perce\n",
    "    else:\n",
    "        perce = perce*100\n",
    "\n",
    "    label_counts = df[id_column].value_counts()\n",
    "\n",
    "    median_count = label_counts.median()\n",
    "    #mantengo solo i dati che hanno almeno il perce% in più o in meno di dati rispetto la mediana\n",
    "\n",
    "    upper_limit = median_count * (1+(perce/100))\n",
    "    lower_limit = median_count * (1-(perce/100))\n",
    "\n",
    "\n",
    "    #filtro\n",
    "    valid_classes = label_counts[(label_counts >= lower_limit) & (label_counts <= upper_limit)].index\n",
    "\n",
    "    # Mantenere solo i dati delle classi valide\n",
    "    cutted_df = df[df[id_column].isin(valid_classes)]\n",
    "\n",
    "    print(label_counts[label_counts >= upper_limit].index)\n",
    "    # Per le classi che eccedono il (100+perc)%, ridurre casualmente i dati\n",
    "    exceeding_classes = label_counts[label_counts >= upper_limit].index\n",
    "\n",
    "    sampled_df_list=[]\n",
    "\n",
    "    for label in exceeding_classes:\n",
    "        class_data = df[df[id_column] == label]\n",
    "        sampled_data = class_data.sample(n=int(upper_limit), random_state=42)\n",
    "        #print(sampled_data)\n",
    "        sampled_df_list.append(sampled_data)\n",
    "\n",
    "        sampled_df = pd.concat(sampled_df_list, ignore_index=True)\n",
    "        final_df = pd.concat([cutted_df, sampled_df], ignore_index=True)\n",
    "\n",
    "    final_df = final_df.sort_values(by='super_category_id', ascending=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def map_labels_based_on_column(df, column_name):\n",
    "    unique_values = sorted(df[column_name].unique())\n",
    "    mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "    labels_old = df[column_name].values\n",
    "    labels = [mapping[x] for x in labels_old]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Valuto se la GPU è disponibile \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chapter_title</th>\n",
       "      <th>super_category_id</th>\n",
       "      <th>super_category_name</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph_title</th>\n",
       "      <th>paragraph_body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>paragraph_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>272</td>\n",
       "      <td>\"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SOCIETARIO</td>\n",
       "      <td>3</td>\n",
       "      <td>TRASFORMAZIONI OMOGENEE DI SOCIETA'</td>\n",
       "      <td>4267</td>\n",
       "      <td>Formule</td>\n",
       "      <td>&amp;nbsp;Repertorio n.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nb...</td>\n",
       "      <td></td>\n",
       "      <td>2023-06-05T20:06:09.000000Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>\"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SOCIETARIO</td>\n",
       "      <td>3</td>\n",
       "      <td>TRASFORMAZIONI OMOGENEE DI SOCIETA'</td>\n",
       "      <td>4269</td>\n",
       "      <td>Nozioni fiscali</td>\n",
       "      <td>Le modifiche dello statuto sociale di società ...</td>\n",
       "      <td></td>\n",
       "      <td>2023-06-05T20:06:09.000000Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272</td>\n",
       "      <td>\"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SOCIETARIO</td>\n",
       "      <td>3</td>\n",
       "      <td>TRASFORMAZIONI OMOGENEE DI SOCIETA'</td>\n",
       "      <td>4272</td>\n",
       "      <td>Documenti da richiedere per l'istruttoria dell...</td>\n",
       "      <td>SOCIETA' DI CUI SI RICEVE L'ATTO DI ADOZIONE D...</td>\n",
       "      <td>4270</td>\n",
       "      <td>2023-06-05T20:06:09.000000Z</td>\n",
       "      <td></td>\n",
       "      <td>Pre-stipula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>\"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SOCIETARIO</td>\n",
       "      <td>3</td>\n",
       "      <td>TRASFORMAZIONI OMOGENEE DI SOCIETA'</td>\n",
       "      <td>4273</td>\n",
       "      <td>Attività pre-stipula da effettuare</td>\n",
       "      <td>PREVENTIVO E INCARICO&lt;ul&gt;&lt;li class=\"ql-align-j...</td>\n",
       "      <td>4270</td>\n",
       "      <td>2023-06-05T20:29:44.000000Z</td>\n",
       "      <td></td>\n",
       "      <td>Pre-stipula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272</td>\n",
       "      <td>\"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...</td>\n",
       "      <td>1</td>\n",
       "      <td>SOCIETARIO</td>\n",
       "      <td>3</td>\n",
       "      <td>TRASFORMAZIONI OMOGENEE DI SOCIETA'</td>\n",
       "      <td>4276</td>\n",
       "      <td>Repertoriazione</td>\n",
       "      <td>FORMA DELL'ATTOAtto pubblico&lt;br&gt;NUMERI PROGRES...</td>\n",
       "      <td>4271</td>\n",
       "      <td>2023-06-05T20:06:09.000000Z</td>\n",
       "      <td></td>\n",
       "      <td>Post stipula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>431</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>8</td>\n",
       "      <td>LA FORMA DEGLI ATTI NOTARILI</td>\n",
       "      <td>57</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>7850</td>\n",
       "      <td>Costituzione S.R.L. ON-LINE modello standard i...</td>\n",
       "      <td>&lt;p class=\"ql-align-justify\"&gt;Repertorio n.&amp;nbsp...</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>2023-09-20 17:00:46</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>431</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>8</td>\n",
       "      <td>LA FORMA DEGLI ATTI NOTARILI</td>\n",
       "      <td>57</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>7851</td>\n",
       "      <td>Costituzione S.R.L. ON-LINE modello standard i...</td>\n",
       "      <td>&lt;p class=\"ql-align-justify\"&gt;Record n.&amp;nbsp;&amp;nb...</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>2023-09-20 17:00:46</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>431</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>8</td>\n",
       "      <td>LA FORMA DEGLI ATTI NOTARILI</td>\n",
       "      <td>57</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>7852</td>\n",
       "      <td>Costituzione S.R.L.S. ON-LINE modello standard...</td>\n",
       "      <td>&lt;p class=\"ql-align-justify\"&gt;Repertorio n.&amp;nbsp...</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>2023-09-20 17:00:46</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>431</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>8</td>\n",
       "      <td>LA FORMA DEGLI ATTI NOTARILI</td>\n",
       "      <td>57</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>7853</td>\n",
       "      <td>Costituzione S.R.L.S. ON-LINE modello standard...</td>\n",
       "      <td>&lt;p class=\"ql-align-justify\"&gt; Record n.&amp;nbsp;&amp;n...</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>2023-09-20 17:00:46</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>431</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>8</td>\n",
       "      <td>LA FORMA DEGLI ATTI NOTARILI</td>\n",
       "      <td>57</td>\n",
       "      <td>STRUTTURA DEGLI ATTI SOCIETARI</td>\n",
       "      <td>7854</td>\n",
       "      <td>Costituzione S.R.L.S. ON-LINE modello standard...</td>\n",
       "      <td>&lt;p class=\"ql-align-justify\"&gt;Repertorio n.&amp;nbsp...</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>2023-09-20 17:00:46</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4618 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                      chapter_title  \\\n",
       "0     272  \"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...   \n",
       "1     272  \"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...   \n",
       "2     272  \"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...   \n",
       "3     272  \"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...   \n",
       "4     272  \"TRASFORMAZIONE\" DA SOCIETA' A RESPONSABILITA'...   \n",
       "...   ...                                                ...   \n",
       "4613  431                     STRUTTURA DEGLI ATTI SOCIETARI   \n",
       "4614  431                     STRUTTURA DEGLI ATTI SOCIETARI   \n",
       "4615  431                     STRUTTURA DEGLI ATTI SOCIETARI   \n",
       "4616  431                     STRUTTURA DEGLI ATTI SOCIETARI   \n",
       "4617  431                     STRUTTURA DEGLI ATTI SOCIETARI   \n",
       "\n",
       "      super_category_id           super_category_name category_id  \\\n",
       "0                     1                    SOCIETARIO           3   \n",
       "1                     1                    SOCIETARIO           3   \n",
       "2                     1                    SOCIETARIO           3   \n",
       "3                     1                    SOCIETARIO           3   \n",
       "4                     1                    SOCIETARIO           3   \n",
       "...                 ...                           ...         ...   \n",
       "4613                  8  LA FORMA DEGLI ATTI NOTARILI          57   \n",
       "4614                  8  LA FORMA DEGLI ATTI NOTARILI          57   \n",
       "4615                  8  LA FORMA DEGLI ATTI NOTARILI          57   \n",
       "4616                  8  LA FORMA DEGLI ATTI NOTARILI          57   \n",
       "4617                  8  LA FORMA DEGLI ATTI NOTARILI          57   \n",
       "\n",
       "                            category_name paragraph_id  \\\n",
       "0     TRASFORMAZIONI OMOGENEE DI SOCIETA'         4267   \n",
       "1     TRASFORMAZIONI OMOGENEE DI SOCIETA'         4269   \n",
       "2     TRASFORMAZIONI OMOGENEE DI SOCIETA'         4272   \n",
       "3     TRASFORMAZIONI OMOGENEE DI SOCIETA'         4273   \n",
       "4     TRASFORMAZIONI OMOGENEE DI SOCIETA'         4276   \n",
       "...                                   ...          ...   \n",
       "4613       STRUTTURA DEGLI ATTI SOCIETARI         7850   \n",
       "4614       STRUTTURA DEGLI ATTI SOCIETARI         7851   \n",
       "4615       STRUTTURA DEGLI ATTI SOCIETARI         7852   \n",
       "4616       STRUTTURA DEGLI ATTI SOCIETARI         7853   \n",
       "4617       STRUTTURA DEGLI ATTI SOCIETARI         7854   \n",
       "\n",
       "                                        paragraph_title  \\\n",
       "0                                               Formule   \n",
       "1                                       Nozioni fiscali   \n",
       "2     Documenti da richiedere per l'istruttoria dell...   \n",
       "3                    Attività pre-stipula da effettuare   \n",
       "4                                       Repertoriazione   \n",
       "...                                                 ...   \n",
       "4613  Costituzione S.R.L. ON-LINE modello standard i...   \n",
       "4614  Costituzione S.R.L. ON-LINE modello standard i...   \n",
       "4615  Costituzione S.R.L.S. ON-LINE modello standard...   \n",
       "4616  Costituzione S.R.L.S. ON-LINE modello standard...   \n",
       "4617  Costituzione S.R.L.S. ON-LINE modello standard...   \n",
       "\n",
       "                                         paragraph_body parent_id  \\\n",
       "0     &nbsp;Repertorio n.&nbsp;&nbsp;&nbsp;&nbsp;&nb...             \n",
       "1     Le modifiche dello statuto sociale di società ...             \n",
       "2     SOCIETA' DI CUI SI RICEVE L'ATTO DI ADOZIONE D...      4270   \n",
       "3     PREVENTIVO E INCARICO<ul><li class=\"ql-align-j...      4270   \n",
       "4     FORMA DELL'ATTOAtto pubblico<br>NUMERI PROGRES...      4271   \n",
       "...                                                 ...       ...   \n",
       "4613  <p class=\"ql-align-justify\">Repertorio n.&nbsp...    6671.0   \n",
       "4614  <p class=\"ql-align-justify\">Record n.&nbsp;&nb...    6671.0   \n",
       "4615  <p class=\"ql-align-justify\">Repertorio n.&nbsp...    6671.0   \n",
       "4616  <p class=\"ql-align-justify\"> Record n.&nbsp;&n...    6671.0   \n",
       "4617  <p class=\"ql-align-justify\">Repertorio n.&nbsp...    6671.0   \n",
       "\n",
       "                       updated_at deleted_at paragraph_tag  \n",
       "0     2023-06-05T20:06:09.000000Z                           \n",
       "1     2023-06-05T20:06:09.000000Z                           \n",
       "2     2023-06-05T20:06:09.000000Z              Pre-stipula  \n",
       "3     2023-06-05T20:29:44.000000Z              Pre-stipula  \n",
       "4     2023-06-05T20:06:09.000000Z             Post stipula  \n",
       "...                           ...        ...           ...  \n",
       "4613          2023-09-20 17:00:46       None           NaN  \n",
       "4614          2023-09-20 17:00:46       None           NaN  \n",
       "4615          2023-09-20 17:00:46       None           NaN  \n",
       "4616          2023-09-20 17:00:46       None           NaN  \n",
       "4617          2023-09-20 17:00:46       None           NaN  \n",
       "\n",
       "[4618 rows x 13 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combino i dati contenuti nei jsons in unico dataframe\n",
    "folder_path = '.\\data'\n",
    "combined_df = combine_json(folder_path)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df['super_category_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(combined_df['super_category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulisco il testo \n",
    "combined_df['paragraph_body'] = combined_df['paragraph_body'].apply(lambda x: clean_text(x, 'italian'))\n",
    "#set(combined_df['super_category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido i testi in chunk di massimo 512 caratteri (massimo accettato da BERT)\n",
    "chunked_combined_df = split_and_chunk_dataframe(combined_df, 'paragraph_body', 'super_category_id', 'super_category_name', 512)\n",
    "#set(chunked_combined_df['super_category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "super_category_id\n",
       "1     20950\n",
       "11     6050\n",
       "2      4054\n",
       "5      4052\n",
       "6      2209\n",
       "10     1674\n",
       "8      1167\n",
       "12      421\n",
       "3        20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_combined_df['super_category_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([1, 11, 2, 5], dtype='int64', name='super_category_id')\n"
     ]
    }
   ],
   "source": [
    "# Bilancio le classi: rimuovo le classi che hanno più o meno del tot% rispetto alla MEDIANA\n",
    "perc = 0.7\n",
    "final_df = balancing_df(chunked_combined_df, 'super_category_id', perc)\n",
    "#set(final_df['super_category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappo le label in modo tale che siano in range [0, #categorie-1]\n",
    "labels = map_labels_based_on_column(final_df, 'super_category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['paragraph_body'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provo ad aggiungere token dal corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(document, nlp=nlp):\n",
    "    # tokenize the document with spaCY\n",
    "    doc = nlp(document)\n",
    "    # Remove stop words and punctuation symbols\n",
    "    tokens = [\n",
    "        token.text for token in doc #if (\n",
    "        #token.is_stop == False and \\\n",
    "        #token.is_punct == False and \\\n",
    "        #token.text.strip() != '' and \\\n",
    "        #token.text.find(\"\\n\") == -1)\n",
    "        ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, tokenizer=spacy_tokenizer, \n",
    "norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input{‘filename’, ‘file’, ‘content’}, default=’content’\n",
    "# If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.\n",
    "# If 'file', the sequence items must have a ‘read’ method (file-like object) that is called to fetch the bytes in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "length = len(final_df)\n",
    "result = tfidf_vectorizer.fit_transform(final_df[\"paragraph_body\"])\n",
    "#1.36s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf of tokens\n",
    "idf = tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens from most frequent in documents to least frequent\n",
    "idf_sorted_indexes = sorted(range(len(idf)), key=lambda k: idf[k])\n",
    "idf_sorted = idf[idf_sorted_indexes]\n",
    "tokens_by_df = np.array(tfidf_vectorizer.get_feature_names_out())[idf_sorted_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the proportion of new tokens to add in vocabulary\n",
    "pct = 10 # all tokens present in at least 10%\n",
    "index_max = len(np.array(idf)[np.array(idf)>=pct])\n",
    "new_tokens = tokens_by_df[:index_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "#tokenizer =  AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "#model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=7, output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = tokenizer.add_tokens(new_tokens.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(43725, 768, padding_idx=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize the embeddings matrix of the model \n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine test enriched vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuto qual è la massima lunghezza delle frasi (sarà 512)\n",
    "\n",
    "#max_len = 0\n",
    "#\n",
    "#for sent in final_df.paragraph_body:\n",
    "#    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "#    if len(input_ids) > max_len:\n",
    "#        max_len = len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Andrea.Bagante\\.virtualenvs\\ai-model-omnia-QHASXsB0\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loop per creare i token e calcolare gli embeddings tramite BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every text...\n",
    "for text in final_df.paragraph_body:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.    '''CAMBIATO A FALSE'''\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20070"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrea.Bagante\\AppData\\Local\\Temp\\ipykernel_214556\\4149322787.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
      "C:\\Users\\Andrea.Bagante\\AppData\\Local\\Temp\\ipykernel_214556\\4149322787.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(attention_masks, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Trasformo gli embedding, le attention mask e le labels in tensori: devono essere salvati cossì per allenare i modelli con pytorch\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=torch.float64)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo i dataset di training, validation e testing con la proporzionalità scelta. Se il testset non avesse le labels, dovrei creare un TensorDataset specifico per lui.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = 0.7\n",
    "validation_size = 0.2\n",
    "testing_size = 0.1\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, validation_size, testing_size])\n",
    "\n",
    "batch_size = 16   # è il numero di campioni utilizzati in un forward e backward pass attraverso la rete. 32 dà problemi di memoria    \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# train_dataloader creato nella parte di testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico il modello BertForSequenceClassification, il modello pretrained di BERT con un singolo layer finale di classificazione\n",
    "#model = BertForSequenceClassification.from_pretrained(\n",
    "#    \"bert-base-uncased\",            # Scelgo il modello tra quelli disponibili (cambiano alcuni layer).\n",
    "#    num_labels = 7,                 # Numero di output del modello, da cambiare in base al numero di categorie.\n",
    "#    output_attentions = False,      \n",
    "#    output_hidden_states = False,   \n",
    "#)\n",
    "\n",
    "# Carico il modello sulla GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loops\n",
    "\n",
    "epochs = 4 #Solitamente se ne mettono molte e si usano i callbacks con gli early_stoppings: fermano l'allenamento quando non apprende più e sta per overfittare.\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "\n",
    "    # Training\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack del batch di training dal dataloader, i tensori vengono copiati sulla GPU \n",
    "        # il batch contiene tre tensori':\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)   \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clippa la norma del gradiente a 1.0. Serve per prevenire il problema dell'\"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #da capire questo\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calcolo della loss media attraverso tutti i batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Controllo quanto tempo ha impiegato questa epoca di allenamento.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    " \n",
    " \n",
    "    # Validation\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():   # non mi serve calcolare il gradiente perché il training è già stato fatto     \n",
    "            output= model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        loss = output.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()  # distacco dal batch i risultati e li salvo sulla CPU\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids) # salvo l'accuracy del batch\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader) # media delle accuracy (accuracy di ogni batch/#tot dei batch)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    # Se l'accuracy dell'epoca attuale è migliore, salvo lo state_dict del modello (salvo i \"parametri\" da utilizzare nel test)\n",
    "    if avg_val_accuracy > best_eval_accuracy:\n",
    "        torch.save(model.state_dict(), './models/bert_model.pth')\n",
    "        best_eval_accuracy = avg_val_accuracy\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# Carico lo state_dict del modello\n",
    "model.load_state_dict(torch.load('./models/bert_model.pth'))\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predictions.extend(list(pred_flat))\n",
    "\n",
    "test_label = test_dataset[:][2].to('cpu').numpy()   #si può fare in altro modo, ad es come nel validation set\n",
    "test_pred = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostro alcune metriche\n",
    "\n",
    "#test_accuracy = flat_accuracy(test_pred, test_label)\n",
    "cm = confusion_matrix(test_label, test_pred, labels=labels.unique())\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_df.super_category_name.unique())\n",
    "disp.plot() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_label, test_pred, target_names=final_df.super_category_name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_and_classification-liCwZ7IT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
