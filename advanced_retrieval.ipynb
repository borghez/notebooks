{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since m-ric/huggingface_doc couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Andrea.Bagante\\.cache\\huggingface\\datasets\\m-ric___huggingface_doc\\default\\0.0.0\\1b83935099b148190b6a9a9874b7e62a17fea889 (last modified on Tue Dec 24 09:12:39 2024).\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2647/2647 [00:00<00:00, 4803.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]}) for doc in knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split docs and keep only unique ones\n",
    "print(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_docs):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[new_doc.page_content] = True\n",
    "            docs_processed.append(new_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrea.Bagante\\AppData\\Local\\Temp\\ipykernel_15600\\689275156.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents... This should take a few minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding documents... This should take a few minutes\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-small\",\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}, \n",
    "    )\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"How to create a pipeline object?\"\n",
    "query_vector = embedding_model.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting retrieval for user_query='How to create a pipeline object?'...\n",
      "\n",
      "==================================Top document==================================\n",
      "## Available Pipelines:\n",
      "==================================Metadata==================================\n",
      "{'source': 'diffusers', 'start_index': 1782}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = vectordb.similarity_search(query=user_query, k=5)\n",
    "print(\"\\n==================================Top document==================================\")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generation_pipeline  = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 8 (Four apples and four oranges make eight fruits.)\\n\\nWhat is 3+3? Answer: 6 (Three apples and three pears make six fruits.)\\n\\nWhat is 5+1? Answer: 6 (Five flowers and one bush make six plants.)\\n\\nWhat is 2+2? Answer: 4 (Two birds and two nests make four groups.)\\n\\nWhat is 7+3? Answer: 10 (Seven trees and three benches make ten pieces of outdoor equipment.)\\n\\nWhat is 6+1? Answer: 7 (Six cars and one traffic light make seven things you see on the street.)\\n\\nWhat is 9+1? Answer: 10 (Nine building and one stop sign make ten things you see in a town.)\\n\\nWhat is 8+2? Answer: 10 (Eight telephones and two computers make ten office supplies.)\\n\\nWhat is 10+0? Answer: 10 (Ten fingers on your hands make ten body parts.)\\n\\nWhat is 5+5? Answer: 10 (Five legs on each ant makes ten legs on ten ants.)\\n\\nWhat is 10+0? Answer: 10 (Ten toes on your feet make ten body parts.)\\n\\nWhat is 10+0? Answer: 10 (Ten pets in your house make ten animals.)\\n\\nWhat is 10+0? Answer: 10 (Ten seeds in a pumpkin make ten things you grow in a garden.)\\n\\nWhat is 10+0? Answer: 10 (Ten stars in the flag make ten things you see on Independence Day.)\\n\\nWhat is 10+0? Answer: 10 (Ten fingers on each hand make ten things you use to play an instrument.)\\n\\nWhat is 10+0? Answer: 10 (Ten wheels on the school bus make ten round things you might spin.)\\n\\nWhat is 10+0? Answer: 10 (Ten petals on each flower make ten things that might be red, white, and yellow.)\\n\\nWhat is 10+0? Answer: 10 (Ten keys on the typewriter make ten things you push.)\\n\\nWhat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_pipeline(\"What is 4+4? Answer:\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Using the information contained in the context,\n",
      "give a comprehensive answer to the question.\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "Provide the number of the source document when relevant.\n",
      "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
      "<|user|>\n",
      "Context:\n",
      "{context}\n",
      "---\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: {question}</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a pipeline object, you can follow these steps:\n",
      "\n",
      "1. For predefined pipelines provided by Hugging Face, you can directly import and use them as shown in Document 1. Here, we are using the `tiny-random-wav2vec2` model for speech recognition. ```python\n",
      "import transformers\n",
      "from transformers import KeyDataset\n",
      "\n",
      "pipe = transformers.pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
      "dataset = transformers.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
      "\n",
      "for out in pipe(KeyDataset(dataset, \"audio\")):\n",
      "    print(out)\n",
      " ```\n",
      "\n",
      "2. If you want to create your own pipeline with custom components, you can define a subclass of `Pipeline` and pass it to the `pipeline()` function along with the required models and tokens. Here, we are creating a simple pipeline for text classification. ```python\n",
      "import torch\n",
      "from transformers import AutoTokenizer, BertForSequenceClassification\n",
      "\n",
      "class TextClassificationPipeline(transformers.Pipeline):\n",
      "    def __init__(self, model: BertForSequenceClassification, tokenizer: AutoTokenizer):\n",
      "        self.model = model\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    def __call__(self, inputs: str) -> dict:\n",
      "        input_ids = self.tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
      "        outputs = self.model(input_ids)\n",
      "        return {\"labels\": outputs.logits}\n",
      "\n",
      "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "my_pipeline = TextClassificationPipeline(model, tokenizer)\n",
      "results = my_pipeline(\"This is a sample text.\")\n",
      "print(results[\"labels\"])\n",
      "```\n",
      "\n",
      "In this example, we first import the necessary libraries and classes. We then define our custom pipeline by inheriting from the `Pipeline` class and overriding its constructor and `__call__()` method. In the constructor, we initialize the required models and tokens. In the `__\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs_text = [doc.page_content for doc in retrieved_docs]  # We only need the text of the documents\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
    "\n",
    "final_prompt = prompt.format(question=\"How to create a pipeline object?\", context=context)\n",
    "\n",
    "# Redact an answer\n",
    "answer = text_generation_pipeline(final_prompt)[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\colbert\\utils\\amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: vectordb,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = prompt.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to create a pipeline object?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea.Bagante\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\retrieval-augmented-sjGhpBXj-py3.11\\Lib\\site-packages\\colbert\\utils\\amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = answer_with_rag(question, text_generation_pipeline, vectordb, reranker=reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Answer==================================\n",
      "To create a pipeline object, follow these steps:\n",
      "\n",
      "1. Import the necessary modules from Hugging Face Transformers library:\n",
      "\n",
      "   ```python\n",
      "   from transformers import pipeline\n",
      "   ```\n",
      "\n",
      "2. Instantiate the pipeline object by passing the name or path of the pretrained model and the desired task as arguments to the `pipeline()` function. For example, to create a pipeline for named entity recognition using the `hf-internal-testing/bert-base-cased` model, you would write:\n",
      "\n",
      "   ```python\n",
      "   pipe = pipeline(model=\"hf-internal-testing/bert-base-cased\", task=\"ner\")\n",
      "   ```\n",
      "\n",
      "3. Load the dataset you want to process using the `load_dataset()` function provided by the library. Here's an example:\n",
      "\n",
      "   ```python\n",
      "   from transformers import AutoTokenizer\n",
      "   from datasets import load_dataset\n",
      "\n",
      "   tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/bert-base-cased\")\n",
      "   dataset = load_dataset(\"gluonnlp/wikitext-103-en\", \"train[:500]\")\n",
      "   ```\n",
      "\n",
      "4. Pass the loaded dataset to the pipeline object to perform inference. The `pipeline()` function returns a generator that yields the output predictions for each input sample:\n",
      "\n",
      "   ```python\n",
      "   for result in pipe(dataset):\n",
      "       # Access the predicted entities and their types\n",
      "       entities = result[\"entity_set\"]\n",
      "       types = result[\"entities\"]\n",
      "      ...\n",
      "   ```\n",
      "\n",
      "That's it! With this simple setup, you can easily use pretrained models for various NLP tasks without having to deal with the underlying implementation details.\n",
      "==================================Source docs==================================\n",
      "Document 0------------------------------------------------------------\n",
      "-->\n",
      "\n",
      "# Pipelines\n",
      "\n",
      "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of\n",
      "the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity\n",
      "Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the\n",
      "[task summary](../task_summary) for examples of use.\n",
      "\n",
      "There are two categories of pipeline abstractions to be aware about:\n",
      "Document 1------------------------------------------------------------\n",
      "pipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
      "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
      "\n",
      "for out in pipe(KeyDataset(dataset, \"audio\")):\n",
      "    print(out)\n",
      "```\n",
      "\n",
      "\n",
      "## Using pipelines for a webserver\n",
      "\n",
      "<Tip>\n",
      "Creating an inference engine is a complex topic which deserves it's own\n",
      "page.\n",
      "</Tip>\n",
      "\n",
      "[Link](./pipeline_webserver)\n",
      "\n",
      "## Vision pipeline\n",
      "\n",
      "Using a [`pipeline`] for vision tasks is practically identical.\n",
      "Document 2------------------------------------------------------------\n",
      "# create pipeline\n",
      "gen = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,device=0)\n",
      "\n",
      "# run prediction\n",
      "gen(\"My Name is philipp\")\n",
      "#[{'generated_text': 'My Name is philipp k. and I live just outside of Detroit....\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Create `model.tar.gz` for the Amazon SageMaker real-time endpoint\n",
      "\n",
      "Since we can load our model quickly and run inference on it letâ€™s deploy it to Amazon SageMaker.\n",
      "Document 3------------------------------------------------------------\n",
      "## Add the pipeline to ðŸ¤— Transformers\n",
      "\n",
      "If you want to contribute your pipeline to ðŸ¤— Transformers, you will need to add a new module in the `pipelines` submodule\n",
      "with the code of your pipeline, then add it to the list of tasks defined in `pipelines/__init__.py`.\n",
      "\n",
      "Then you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py` with examples of the other tests.\n",
      "Document 4------------------------------------------------------------\n",
      "## Pipeline\n",
      "\n",
      "<Youtube id=\"tiZFewofSLM\"/>\n",
      "\n",
      "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
      "\n",
      "</Tip>\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================Answer==================================\")\n",
    "print(f\"{answer}\")\n",
    "print(\"==================================Source docs==================================\")\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Document {i}------------------------------------------------------------\")\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-augmented-sjGhpBXj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
